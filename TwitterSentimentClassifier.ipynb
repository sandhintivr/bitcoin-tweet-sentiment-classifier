{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTweetData():\n",
    "    sentiment = []\n",
    "    tweet = []\n",
    "    with open('nbs/data/Sentiment-Analysis-Dataset.csv') as text:\n",
    "        for line in text:\n",
    "            row = line.split(',')\n",
    "            sentiment.append(row[1])\n",
    "            tweet.append(row[3])\n",
    "        \n",
    "    del sentiment[0]\n",
    "    del tweet[0]\n",
    "    print len(sentiment)\n",
    "    print len(tweet)\n",
    "    \n",
    "    return sentiment, tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    for i in range(0, len(tweet) - 1):\n",
    "        #lower case and to string\n",
    "        tweet[i] = str(tweet[i]).lower()\n",
    "        #links to URL\n",
    "        tweet[i] = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet[i])\n",
    "        #username to AT_USER\n",
    "        tweet[i] = re.sub('@[^\\s]+','AT_USER',tweet[i])\n",
    "        #Remove white spaces\n",
    "        tweet[i] = re.sub('[\\s]+', ' ', tweet[i])\n",
    "        #replace #word with word\n",
    "        tweet[i] = re.sub(r'#([^\\s]+)', r'\\1', tweet[i])\n",
    "        #trim\n",
    "        tweet[i] = tweet[i].strip('\\'\"')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWordList(fileName):\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "   \n",
    "    StopWordFile = open(fileName, 'r')\n",
    "    line = StopWordFile.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = StopWordFile.readline()\n",
    "    StopWordFile.close()\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []\n",
    "    words = str(tweet).split()\n",
    "    for word in words:\n",
    "        word = word.strip('\\'\"?,.')\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", word)\n",
    "        if(word in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(word.lower())\n",
    "            \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578627\n",
      "1578627\n"
     ]
    }
   ],
   "source": [
    "full_tweets = []\n",
    "featureList = []\n",
    "\n",
    "sentiment, tweet = getTweetData()\n",
    "clean_tweet = cleanTweet(tweet)\n",
    "stopWords = getStopWordList('nbs/data/stopwords.txt')\n",
    "for i in range(0, len(clean_tweet) - 1):\n",
    "    featureVector = getFeatureVector(clean_tweet[i], stopWords)\n",
    "    featureList.extend(featureVector)\n",
    "    full_tweets.append((featureVector, sentiment[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureList = list(set(featureList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214790"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sad',\n",
       " 'apl',\n",
       " 'friend',\n",
       " 'missed',\n",
       " 'moon',\n",
       " 'trailer',\n",
       " 'omg',\n",
       " 'omgaga',\n",
       " 'im',\n",
       " 'sooo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zzzzzzzzzzzzz', 'meh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sad', 'apl', 'friend'], '0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.util.apply_features(extract_features, full_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTweet = 'Congrats @brodie, this is some COOL analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processedTestTweet = processTweet(testTweet)\n",
    "print NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
